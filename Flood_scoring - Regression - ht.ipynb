{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8feeb819",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370efd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir(r\"E:\\Data challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c369be",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876c87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_drias = pd.read_csv(r\"data\\Drias_data\\RCP_4.5.csv\", sep=\";\", header=31)\n",
    "data_drias = pd.read_csv(r\"data\\Drias_data\\RCP_4.5_with_distance.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75756b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Flood = pd.read_csv(r\"data\\Flood\\flood_risk_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed832f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clay = gpd.read_file(r\"data\\Flood\\clay_risk_results.csv\")\n",
    "data_clay = data_clay.astype({'longitude': 'Float64', 'latitude': 'Float64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91ce72",
   "metadata": {},
   "source": [
    "# Preparing Data Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11a1495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_8960\\2279142045.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_clay[\"alea\"] = data_clay[\"alea\"].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "data_clay[\"alea\"] = data_clay[\"alea\"].replace({\"Faible\": 1,\"Moyen\": 2,\"Fort\": 3})\n",
    "data_clay.drop(columns=['niveau'],inplace=True)\n",
    "data_clay[\"alea\"] = data_clay[\"alea\"].replace(\"\", pd.NA)\n",
    "data_clay[\"alea\"] = data_clay[\"alea\"].fillna(0)\n",
    "data_clay = data_clay.astype({'alea': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a941ed27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>scenario</th>\n",
       "      <th>ht_min</th>\n",
       "      <th>ht_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>8.6413</td>\n",
       "      <td>41.9452</td>\n",
       "      <td>High</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>8.7372</td>\n",
       "      <td>41.9394</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>8.8330</td>\n",
       "      <td>41.9335</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>8.8409</td>\n",
       "      <td>42.0051</td>\n",
       "      <td>High</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>2.8711</td>\n",
       "      <td>42.5634</td>\n",
       "      <td>High</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26908</th>\n",
       "      <td>26908</td>\n",
       "      <td>3.0718</td>\n",
       "      <td>50.7606</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26915</th>\n",
       "      <td>26915</td>\n",
       "      <td>2.1665</td>\n",
       "      <td>50.8345</td>\n",
       "      <td>High</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26930</th>\n",
       "      <td>26930</td>\n",
       "      <td>1.8252</td>\n",
       "      <td>50.9770</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26931</th>\n",
       "      <td>26931</td>\n",
       "      <td>1.9388</td>\n",
       "      <td>50.9774</td>\n",
       "      <td>High</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26940</th>\n",
       "      <td>26940</td>\n",
       "      <td>2.3934</td>\n",
       "      <td>51.0497</td>\n",
       "      <td>High</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1545 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  longitude  latitude scenario  ht_min  ht_max\n",
       "47             47     8.6413   41.9452     High     0.5     1.0\n",
       "48             48     8.7372   41.9394     High     2.0     4.0\n",
       "49             49     8.8330   41.9335     High     2.0     4.0\n",
       "58             58     8.8409   42.0051     High     0.5     1.0\n",
       "160           160     2.8711   42.5634     High     1.0     2.0\n",
       "...           ...        ...       ...      ...     ...     ...\n",
       "26908       26908     3.0718   50.7606     High     2.0  9999.0\n",
       "26915       26915     2.1665   50.8345     High     0.0     0.0\n",
       "26930       26930     1.8252   50.9770     High     2.0  9999.0\n",
       "26931       26931     1.9388   50.9774     High     0.5     1.0\n",
       "26940       26940     2.3934   51.0497     High     0.5     1.0\n",
       "\n",
       "[1545 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Flood_High = data_Flood.copy()\n",
    "data_Flood_High[\"scenario\"] = data_Flood_High[\"scenario\"].apply(lambda x: \"High\" if \"high\" in x else None)\n",
    "data_Flood_High[\"ht\"] = data_Flood_High[\"ht\"].apply(lambda x: eval(x)['high'])\n",
    "data_Flood_High[\"ht\"] = data_Flood_High[\"ht\"].apply(lambda x: max(set([tuple(l) for l in x]), key=x.count, default=None))\n",
    "\n",
    "data_Flood_High[\"ht_min\"] = data_Flood_High[\"ht\"].str[0]\n",
    "data_Flood_High[\"ht_max\"] = data_Flood_High[\"ht\"].str[-1]\n",
    "data_Flood_High.drop(columns=[\"ht\"], inplace=True)\n",
    "\n",
    "data_Flood_High.loc[data_Flood_High['scenario'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b1658a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>scenario</th>\n",
       "      <th>ht_min</th>\n",
       "      <th>ht_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>8.6413</td>\n",
       "      <td>41.9452</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>8.7372</td>\n",
       "      <td>41.9394</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>8.8330</td>\n",
       "      <td>41.9335</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>8.8409</td>\n",
       "      <td>42.0051</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>2.8711</td>\n",
       "      <td>42.5634</td>\n",
       "      <td>Mid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26908</th>\n",
       "      <td>26908</td>\n",
       "      <td>3.0718</td>\n",
       "      <td>50.7606</td>\n",
       "      <td>Mid</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26922</th>\n",
       "      <td>26922</td>\n",
       "      <td>1.8260</td>\n",
       "      <td>50.9053</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26930</th>\n",
       "      <td>26930</td>\n",
       "      <td>1.8252</td>\n",
       "      <td>50.9770</td>\n",
       "      <td>Mid</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26931</th>\n",
       "      <td>26931</td>\n",
       "      <td>1.9388</td>\n",
       "      <td>50.9774</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26940</th>\n",
       "      <td>26940</td>\n",
       "      <td>2.3934</td>\n",
       "      <td>51.0497</td>\n",
       "      <td>Mid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  longitude  latitude scenario  ht_min  ht_max\n",
       "47             47     8.6413   41.9452      Mid     0.5     1.0\n",
       "48             48     8.7372   41.9394      Mid     0.5     1.0\n",
       "49             49     8.8330   41.9335      Mid     0.5     1.0\n",
       "58             58     8.8409   42.0051      Mid     0.5     1.0\n",
       "160           160     2.8711   42.5634      Mid     1.0     2.0\n",
       "...           ...        ...       ...      ...     ...     ...\n",
       "26908       26908     3.0718   50.7606      Mid     2.0  9999.0\n",
       "26922       26922     1.8260   50.9053      Mid     0.0     0.5\n",
       "26930       26930     1.8252   50.9770      Mid     2.0  9999.0\n",
       "26931       26931     1.9388   50.9774      Mid     0.5     1.0\n",
       "26940       26940     2.3934   51.0497      Mid     0.5     1.0\n",
       "\n",
       "[1776 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Flood_mid = data_Flood.copy()\n",
    "data_Flood_mid[\"scenario\"] = data_Flood_mid[\"scenario\"].apply(lambda x: \"Mid\" if \"mid\" in x else None)\n",
    "data_Flood_mid[\"ht\"] = data_Flood_mid[\"ht\"].apply(lambda x: eval(x)['mid'])\n",
    "data_Flood_mid[\"ht\"] = data_Flood_mid[\"ht\"].apply(lambda x: max(set([tuple(l) for l in x]), key=x.count, default=None))\n",
    "\n",
    "data_Flood_mid[\"ht_min\"] = data_Flood_mid[\"ht\"].str[0]\n",
    "data_Flood_mid[\"ht_max\"] = data_Flood_mid[\"ht\"].str[-1]\n",
    "data_Flood_mid.drop(columns=[\"ht\"], inplace=True)\n",
    "\n",
    "data_Flood_mid.loc[data_Flood_mid['scenario'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4924bf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>scenario</th>\n",
       "      <th>ht_min</th>\n",
       "      <th>ht_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>8.6413</td>\n",
       "      <td>41.9452</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>8.7372</td>\n",
       "      <td>41.9394</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>8.8330</td>\n",
       "      <td>41.9335</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>8.8409</td>\n",
       "      <td>42.0051</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>2.8711</td>\n",
       "      <td>42.5634</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26890</th>\n",
       "      <td>26890</td>\n",
       "      <td>2.8448</td>\n",
       "      <td>50.6900</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26892</th>\n",
       "      <td>26892</td>\n",
       "      <td>3.0707</td>\n",
       "      <td>50.6889</td>\n",
       "      <td>Low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26901</th>\n",
       "      <td>26901</td>\n",
       "      <td>2.2799</td>\n",
       "      <td>50.7628</td>\n",
       "      <td>Low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26905</th>\n",
       "      <td>26905</td>\n",
       "      <td>2.7324</td>\n",
       "      <td>50.7622</td>\n",
       "      <td>Low</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26908</th>\n",
       "      <td>26908</td>\n",
       "      <td>3.0718</td>\n",
       "      <td>50.7606</td>\n",
       "      <td>Low</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1884 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  longitude  latitude scenario  ht_min  ht_max\n",
       "47             47     8.6413   41.9452      Low     0.0     1.0\n",
       "48             48     8.7372   41.9394      Low     0.0     1.0\n",
       "49             49     8.8330   41.9335      Low     0.0     1.0\n",
       "58             58     8.8409   42.0051      Low     0.5     1.0\n",
       "160           160     2.8711   42.5634      Low     1.0     2.0\n",
       "...           ...        ...       ...      ...     ...     ...\n",
       "26890       26890     2.8448   50.6900      Low     0.5     1.0\n",
       "26892       26892     3.0707   50.6889      Low     2.0  9999.0\n",
       "26901       26901     2.2799   50.7628      Low     2.0   999.0\n",
       "26905       26905     2.7324   50.7622      Low     0.5     1.0\n",
       "26908       26908     3.0718   50.7606      Low     2.0  9999.0\n",
       "\n",
       "[1884 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Flood_low = data_Flood.copy()\n",
    "data_Flood_low[\"scenario\"] = data_Flood_low[\"scenario\"].apply(lambda x: \"Low\" if \"low\" in x else None)\n",
    "data_Flood_low[\"ht\"] = data_Flood_low[\"ht\"].apply(lambda x: eval(x)['low'])\n",
    "data_Flood_low[\"ht\"] = data_Flood_low[\"ht\"].apply(lambda x: max(set([tuple(l) for l in x]), key=x.count, default=None))\n",
    "\n",
    "data_Flood_low[\"ht_min\"] = data_Flood_low[\"ht\"].str[0]\n",
    "data_Flood_low[\"ht_max\"] = data_Flood_low[\"ht\"].str[-1]\n",
    "data_Flood_low.drop(columns=[\"ht\"], inplace=True)\n",
    "\n",
    "data_Flood_low.loc[data_Flood_low['scenario'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ea396",
   "metadata": {},
   "source": [
    "# Join our different Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f326c",
   "metadata": {},
   "source": [
    "### concatenate 3 intermediate Flood risk dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b06f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flood_list = [data_Flood_High,data_Flood_mid,data_Flood_low]\n",
    "Data_Flood = pd.concat(Flood_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20466d4a",
   "metadata": {},
   "source": [
    "Join **Flood risk** data with **Drias** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a856b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = Data_Flood.merge(\n",
    "    data_drias[data_drias['Période']=='H1'],\n",
    "    left_on=[\"latitude\", \"longitude\"],\n",
    "    right_on=[\"Latitude\", \"Longitude\"],\n",
    "    how=\"inner\"   # ou \"left\" selon ton besoin\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497d635",
   "metadata": {},
   "source": [
    "Join our **data_merged** dataset with our datas on **clay soils**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4fb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = gpd.GeoDataFrame(data_merged, geometry=gpd.points_from_xy(data_merged.longitude, data_merged.latitude), crs=\"EPSG:4326\")\n",
    "gdf2 = gpd.GeoDataFrame(data_clay, geometry=gpd.points_from_xy(data_clay.longitude, data_clay.latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "gdf1 = gdf1.to_crs(3857)\n",
    "gdf2 = gdf2.to_crs(3857)\n",
    "\n",
    "data_merged = gpd.sjoin_nearest(gdf1,gdf2.drop(columns=['latitude','longitude']),max_distance=50,distance_col=\"distance_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221527ef",
   "metadata": {},
   "source": [
    "**Remove unecessary columns** and **clean Nan** and **categorical** datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c33a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_8960\\828002797.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_merged[\"scenario\"] = data_merged[\"scenario\"].replace({\n"
     ]
    }
   ],
   "source": [
    "#data_merged.drop(columns=['Unnamed: 0','Unnamed: 17','Point','longitude','latitude','ht_min','ht_max','Longitude','Latitude','Contexte','Période','field_1','distance_m','index_right','geometry'], inplace=True)\n",
    "data_merged.drop(columns=['Unnamed: 0','Unnamed: 17','Point','longitude','latitude','Longitude','Latitude','Contexte','Période','field_1','distance_m','index_right','geometry'], inplace=True)\n",
    "data_merged[\"scenario\"] = data_merged[\"scenario\"].replace({\n",
    "    \"Low\": 3,\n",
    "    \"Mid\": 2,\n",
    "    \"High\": 1\n",
    "})\n",
    "data_merged[\"ht_min\"] = data_merged[\"ht_min\"].fillna(0)\n",
    "data_merged[\"ht_max\"] = data_merged[\"ht_max\"].fillna(0)\n",
    "data_merged[\"scenario\"] = data_merged[\"scenario\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44be7f",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85380c0c",
   "metadata": {},
   "source": [
    "Our goal is to predict **ht_min** and **ht_max** using our **Drias** dataset, focusing on **clay soils** and the **Flood risk class**.\\\n",
    "For this purpose, we chose to use a **classification** approach (since both variables have a limited number of classes, 6 for ht_min and 10 for ht_max).\\\n",
    "We applied **weights** to account for the **imbalance** in **class distributions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34b6be",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7a2b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_merged.copy()\n",
    "df['ht_max'] = df['ht_max'].apply(lambda x: x if x <= 6 else 1000)\n",
    "\n",
    "\n",
    "y = df[[\"ht_min\",\"ht_max\"]]\n",
    "X = df.drop(columns=[\"ht_min\",\"ht_max\"])\n",
    "\n",
    "# garder seulement numériques\n",
    "X = X.select_dtypes(include=[\"int32\",\"int64\",\"float64\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "997c017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[['NORPAV', 'NORRR', 'NORRR1MM', 'NORPN20MM', 'NORPFL90',\n",
    "       'NORPXCDD', 'NORPINT', 'NORPQ90', 'NORPQ99', 'NORRR99', 'NORHUSAV',\n",
    "       'NORETPC','dist_fleuve_km','dist_riviere_km','dist_cote_km']] = scaler.fit_transform(X_train[['NORPAV', 'NORRR', 'NORRR1MM', 'NORPN20MM', 'NORPFL90',\n",
    "       'NORPXCDD', 'NORPINT', 'NORPQ90', 'NORPQ99', 'NORRR99', 'NORHUSAV',\n",
    "       'NORETPC','dist_fleuve_km','dist_riviere_km','dist_cote_km']])\n",
    "\n",
    "X_test[['NORPAV', 'NORRR', 'NORRR1MM', 'NORPN20MM', 'NORPFL90',\n",
    "       'NORPXCDD', 'NORPINT', 'NORPQ90', 'NORPQ99', 'NORRR99', 'NORHUSAV',\n",
    "       'NORETPC','dist_fleuve_km','dist_riviere_km','dist_cote_km']] = scaler.transform(X_test[['NORPAV', 'NORRR', 'NORRR1MM', 'NORPN20MM', 'NORPFL90',\n",
    "       'NORPXCDD', 'NORPINT', 'NORPQ90', 'NORPQ99', 'NORRR99', 'NORHUSAV',\n",
    "       'NORETPC','dist_fleuve_km','dist_riviere_km','dist_cote_km']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4c1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb58ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to(torch.int64)\n",
    "y_test  = y_test.to(torch.int64)\n",
    "\n",
    "\n",
    "num_classes_min = y_train[:,0].max().item() + 1  # assuming classes start at 0\n",
    "num_classes_max = y_train[:,1].max().item() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e70c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y_col):\n",
    "    classes, counts = torch.unique(y_col, return_counts=True)\n",
    "    weights = counts.float().sum() / (len(classes) * counts.float())\n",
    "    weight_tensor = torch.zeros(y_col.max()+1)\n",
    "    weight_tensor[classes] = weights\n",
    "    return weight_tensor\n",
    "\n",
    "weight_min = get_class_weights(y_train[:,0])\n",
    "weight_max = get_class_weights(y_train[:,1])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Define the model\n",
    "# -------------------------------\n",
    "class DenseNetClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes_min, num_classes_max):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.ht_min = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes_min)\n",
    "        )\n",
    "        self.ht_max = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes_max)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #base = self.net(x)\n",
    "        return self.ht_min(x), self.ht_max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87547c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 | Train Loss: 8.6349 | Val Loss: 8.4234 | \n",
      "Epoch 0021 | Train Loss: 2.5995 | Val Loss: 2.4514 | \n",
      "Epoch 0041 | Train Loss: 1.6632 | Val Loss: 1.5965 | \n",
      "Epoch 0061 | Train Loss: 1.3390 | Val Loss: 1.3016 | \n",
      "Epoch 0081 | Train Loss: 1.1412 | Val Loss: 1.1034 | \n",
      "Epoch 0101 | Train Loss: 0.9778 | Val Loss: 0.9514 | \n",
      "Epoch 0121 | Train Loss: 0.8506 | Val Loss: 0.8294 | \n",
      "Epoch 0141 | Train Loss: 0.7575 | Val Loss: 0.7381 | \n",
      "Epoch 0161 | Train Loss: 0.6912 | Val Loss: 0.6720 | \n",
      "Epoch 0181 | Train Loss: 0.6381 | Val Loss: 0.6220 | \n",
      "Epoch 0201 | Train Loss: 0.6022 | Val Loss: 0.5880 | \n",
      "Epoch 0221 | Train Loss: 0.5713 | Val Loss: 0.5611 | \n",
      "Epoch 0241 | Train Loss: 0.5497 | Val Loss: 0.5409 | \n",
      "Epoch 0261 | Train Loss: 0.5242 | Val Loss: 0.5234 | \n",
      "Epoch 0281 | Train Loss: 0.5058 | Val Loss: 0.5045 | \n",
      "Epoch 0301 | Train Loss: 0.4883 | Val Loss: 0.4879 | \n",
      "Epoch 0321 | Train Loss: 0.4688 | Val Loss: 0.4730 | \n",
      "Epoch 0341 | Train Loss: 0.4529 | Val Loss: 0.4599 | \n",
      "Epoch 0361 | Train Loss: 0.4398 | Val Loss: 0.4490 | \n",
      "Epoch 0381 | Train Loss: 0.4267 | Val Loss: 0.4327 | \n",
      "Epoch 0401 | Train Loss: 0.4142 | Val Loss: 0.4216 | \n",
      "Epoch 0421 | Train Loss: 0.4029 | Val Loss: 0.4094 | \n",
      "Epoch 0441 | Train Loss: 0.3920 | Val Loss: 0.4022 | \n",
      "Epoch 0461 | Train Loss: 0.3800 | Val Loss: 0.3881 | \n",
      "Epoch 0481 | Train Loss: 0.3685 | Val Loss: 0.3788 | \n"
     ]
    }
   ],
   "source": [
    "model = DenseNetClassifier(X_train.shape[1], num_classes_min, num_classes_max)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Loss and optimizer\n",
    "# -------------------------------\n",
    "criterion_min = nn.CrossEntropyLoss(weight=weight_min)\n",
    "criterion_max = nn.CrossEntropyLoss(weight=weight_max)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training loop\n",
    "# -------------------------------\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ht_min_logits, ht_max_logits = model(X_train)\n",
    "    loss_min = criterion_min(ht_min_logits, y_train[:,0])\n",
    "    loss_max = criterion_max(ht_max_logits, y_train[:,1])\n",
    "    train_loss = loss_min + loss_max\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_min_logits, val_max_logits = model(X_test)\n",
    "        val_loss = criterion_min(val_min_logits, y_test[:,0]) + \\\n",
    "                   criterion_max(val_max_logits, y_test[:,1])\n",
    "\n",
    "        # Accuracy and macro-F1\n",
    "        val_min_pred = torch.argmax(val_min_logits, dim=1)\n",
    "        val_max_pred = torch.argmax(val_max_logits, dim=1)\n",
    "        acc_min = accuracy_score(y_test[:,0].numpy(), val_min_pred.numpy())\n",
    "        acc_max = accuracy_score(y_test[:,1].numpy(), val_max_pred.numpy())\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:04d} | \"\n",
    "              f\"Train Loss: {train_loss.item():.4f} | \"\n",
    "              f\"Val Loss: {val_loss.item():.4f} | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b5092f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ht_min_logits, ht_max_logits = model(X_test)\n",
    "    ht_min_pred = torch.argmax(ht_min_logits, dim=1)\n",
    "    ht_max_pred = torch.argmax(ht_max_logits, dim=1)\n",
    "\n",
    "y_pred_df = pd.DataFrame({\n",
    "    'ht_min': ht_min_pred.numpy(),\n",
    "    'ht_max': ht_max_pred.numpy()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b14214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ht_min: 0.9668302472954211\n",
      "Accuracy ht_max: 0.9763839058655339\n",
      "\n",
      "Classification report ht_min:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     70657\n",
      "           1       0.38      0.96      0.55       337\n",
      "           2       0.48      0.98      0.64      1747\n",
      "           4       0.43      1.00      0.60         6\n",
      "\n",
      "    accuracy                           0.97     72747\n",
      "   macro avg       0.57      0.98      0.69     72747\n",
      "weighted avg       0.98      0.97      0.97     72747\n",
      "\n",
      "\n",
      "Classification report ht_max:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     68389\n",
      "           1       0.76      0.64      0.69      2340\n",
      "           2       0.35      0.91      0.50       284\n",
      "           3       0.52      0.97      0.68       265\n",
      "           4       0.45      1.00      0.62        27\n",
      "           5       1.00      1.00      1.00         3\n",
      "        1000       0.68      0.70      0.69      1439\n",
      "\n",
      "    accuracy                           0.98     72747\n",
      "   macro avg       0.68      0.89      0.74     72747\n",
      "weighted avg       0.98      0.98      0.98     72747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(y_test.numpy(), columns=['ht_min', 'ht_max'])\n",
    "\n",
    "print(\"Accuracy ht_min:\", accuracy_score(y_test_df['ht_min'], y_pred_df['ht_min']))\n",
    "print(\"Accuracy ht_max:\", accuracy_score(y_test_df['ht_max'], y_pred_df['ht_max']))\n",
    "\n",
    "print(\"\\nClassification report ht_min:\\n\", \n",
    "      classification_report(y_test_df['ht_min'], y_pred_df['ht_min']))\n",
    "print(\"\\nClassification report ht_max:\\n\", \n",
    "      classification_report(y_test_df['ht_max'], y_pred_df['ht_max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20209e5a",
   "metadata": {},
   "source": [
    "Our **precision** is decreasing with the number of occurences in the class but our **recall** remains high enough which means that even if our model predict false positive for classes that appear rarely, it still manage to predict true positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1670e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ht_min  ht_max\n",
       "0       0         67976\n",
       "2       1000       1444\n",
       "        1          1343\n",
       "        3           450\n",
       "1       2           414\n",
       "        1           389\n",
       "2       2           317\n",
       "0       1           235\n",
       "        3            38\n",
       "1       1000         34\n",
       "2       4            30\n",
       "0       1000         16\n",
       "        4            16\n",
       "1       4            11\n",
       "0       2            11\n",
       "1       0             4\n",
       "4       1             4\n",
       "2       0             3\n",
       "4       4             3\n",
       "        5             3\n",
       "1       3             2\n",
       "4       3             2\n",
       "        1000          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259a0a9",
   "metadata": {},
   "source": [
    "# Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf8f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, r'model\\model_ht.pth')\n",
    "import joblib\n",
    "joblib.dump(scaler, r\"model\\ht_scaler.pkl\")\n",
    "torch.save(model.state_dict(), r'model\\model_ht_state.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
